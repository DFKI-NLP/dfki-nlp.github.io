[{"authors":["arne-binder"],"categories":null,"content":"","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"db472dfe584699b02c3fb0b542f4efba","permalink":"https://dfki-nlp.github.io/authors/arne-binder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/arne-binder/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Arne Binder","type":"authors"},{"authors":null,"categories":null,"content":"Christoph is now a postdoc in the Machine Learning group at Humboldt University of Berlin.\n","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"55f7fc0a0becc469231bd11edf9d90c1","permalink":"https://dfki-nlp.github.io/authors/christoph-alt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/christoph-alt/","section":"authors","summary":"Christoph is now a postdoc in the Machine Learning group at Humboldt University of Berlin.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["david-harbecke"],"categories":null,"content":"","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"57423a933a38af75c718059324719b6e","permalink":"https://dfki-nlp.github.io/authors/david-harbecke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/david-harbecke/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"David Harbecke","type":"authors"},{"authors":["leonhard-hennig"],"categories":null,"content":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available. As a project lead, I\u0026rsquo;ve managed various national research projects, such as Smart Data Web, PLASS, DAYSTREAM, and the DFKI part in the Berlin Big Data Center, as well as industry-funded projects, e.g. for Deutsche Telekom and Lenovo.\n","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"76acb2a1dbfa728042427546fca4cab6","permalink":"https://dfki-nlp.github.io/authors/leonhard-hennig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leonhard-hennig/","section":"authors","summary":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available.","tags":["Researchers"],"title":"Leonhard Hennig","type":"authors"},{"authors":["philippe-thomas"],"categories":null,"content":"","date":1642673791,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642673791,"objectID":"e87ec0a5f91d896556892e387ce48cc6","permalink":"https://dfki-nlp.github.io/authors/philippe-thomas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/philippe-thomas/","section":"authors","summary":"","tags":["Researchers"],"title":"Philippe Thomas","type":"authors"},{"authors":["robert-schwarzenberg"],"categories":null,"content":"I conducted my PhD research at the Speech and Language Technologies Lab of the German Research Center for Artificial Intelligence (DFKI).\nMy interests include\n (Neural) Explainability Methods and Explainable Models, NLP and NLU, some Image Processing on the side, and Graph Algorithms because, you see, everything seems to be part of some graph.  ","date":1634256e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1634256e3,"objectID":"9c4340557d33d0eef87e7e24354df0fe","permalink":"https://dfki-nlp.github.io/authors/robert-schwarzenberg/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/robert-schwarzenberg/","section":"authors","summary":"I conducted my PhD research at the Speech and Language Technologies Lab of the German Research Center for Artificial Intelligence (DFKI).\nMy interests include\n (Neural) Explainability Methods and Explainable Models, NLP and NLU, some Image Processing on the side, and Graph Algorithms because, you see, everything seems to be part of some graph.","tags":["Alumni"],"title":"Robert Schwarzenberg","type":"authors"},{"authors":["steffen-castle"],"categories":null,"content":"","date":1634256e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1634256e3,"objectID":"ad183e9940c0547e2260bca696fd06ca","permalink":"https://dfki-nlp.github.io/authors/steffen-castle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/steffen-castle/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Steffen Castle","type":"authors"},{"authors":["aleksandra-gabryszak"],"categories":null,"content":"","date":1630972800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630972800,"objectID":"bbfb2073a6e0589384f60ea2fae79732","permalink":"https://dfki-nlp.github.io/authors/aleksandra-gabryszak/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aleksandra-gabryszak/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Aleksandra Gabryszak","type":"authors"},{"authors":["malte-ostendorff"],"categories":null,"content":"In my research work, I mainly focus on information retrieval, recommender systems, and natural language processing. In particular, techniques for the information extraction from unstructured data such as text and making information more accessible are of great interest for me. In my recent work I apply these techniques on content from the legal domain, e.g. laws, patents, case files. Moreover, I explore how recommender systems can assist users in finding relevant information to cope with todayâ€™s information overload. Due to the large amounts of available data, all my work requires the use of scalable and distributed computing. Generally speaking, all topics that are somehow related to the following fields can be considered as my research interest:\n Recommender Systems Natural Language Processing Text Mining Applied Machine Learning Scalable Data Processing (\u0026ldquo;Big Data\u0026rdquo;) Legal Tech  Feel free to contact me if you have any questions regarding my work. I am always open for new ideas, projects and collaborations with other researchers and students.\n","date":1622110683,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622110683,"objectID":"b87fe2d0e7981b54435e6d6bf9861b08","permalink":"https://dfki-nlp.github.io/authors/malte-ostendorff/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/malte-ostendorff/","section":"authors","summary":"In my research work, I mainly focus on information retrieval, recommender systems, and natural language processing. In particular, techniques for the information extraction from unstructured data such as text and making information more accessible are of great interest for me.","tags":["PhD Candidates"],"title":"Malte Ostendorff","type":"authors"},{"authors":["karolina-zaczynska"],"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592870400,"objectID":"e7523b67ec5174035b12fb4d48d29306","permalink":"https://dfki-nlp.github.io/authors/karolina-zaczynska/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karolina-zaczynska/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Karolina Zaczynska","type":"authors"},{"authors":["nils-feldhus"],"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592870400,"objectID":"ad47029271d18471d52333582f6f09d3","permalink":"https://dfki-nlp.github.io/authors/nils-feldhus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nils-feldhus/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Nils Feldhus","type":"authors"},{"authors":["lisa-raithel"],"categories":null,"content":"","date":1569863398,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1569863398,"objectID":"2bf26a380b092795d4a187e33b919ab1","permalink":"https://dfki-nlp.github.io/authors/lisa-raithel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lisa-raithel/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Lisa Raithel","type":"authors"},{"authors":["eleftherios-avramidis"],"categories":null,"content":"","date":1553791245,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1553791245,"objectID":"143f99ce31d6112e2c40d7cec4733d66","permalink":"https://dfki-nlp.github.io/authors/eleftherios-avramidis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/eleftherios-avramidis/","section":"authors","summary":"","tags":["Researchers"],"title":"Eleftherios Avramidis","type":"authors"},{"authors":["he-wang"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1543622400,"objectID":"6b5ebf54ed7db6b1d510ed1b4ccd6c8d","permalink":"https://dfki-nlp.github.io/authors/he-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/he-wang/","section":"authors","summary":"","tags":["Software Engineers"],"title":"He Wang","type":"authors"},{"authors":["roland-roller"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1543622400,"objectID":"5a5a71e8444dfa2557b703df82e80e8d","permalink":"https://dfki-nlp.github.io/authors/roland-roller/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/roland-roller/","section":"authors","summary":"","tags":["Researchers"],"title":"Roland Roller","type":"authors"},{"authors":null,"categories":null,"content":"DFKI-NLP is a Natural Language Processing group of researchers, software engineers and students at the Berlin office of the German Research Center for Artificial Intelligence (DFKI) working on basic and applied research in areas covering, among others, information extraction, knowledge base population, dialogue, sentiment analysis, and summarization. We are particularly interested in core research on learning in low-resource settings, reasoning over larger contexts, and continual learning. We strive for a deeper understanding of human language and thinking, with the goal of developing novel methods for processing and generating human language text, speech, and knowledge. An important part of our work is the creation of corpora, the evaluation of NLP datasets and tasks, and the explainability of (neural) models.\nKey topics:\n Applied / domain-specific information extraction Learning in low-resource settings and over large contexts Construction and analysis of IE datasets, linguistic annotation Multilingual information extraction Evaluation methodology research Explainability  Our group forms a part of DFKI\u0026rsquo;s Speech and Language Technology department led by Prof. Sebastian MÃ¶ller, and closely collaborates with e.g. the Technische UniversitÃ¤t Berlin, DFKI\u0026rsquo;s Language Technology and Multilinguality department and DFKI\u0026rsquo;s Intelligent Analytics for Massive Data group.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"28cf2f802ba8f342b4dc1a3a2cf18f61","permalink":"https://dfki-nlp.github.io/authors/dfki-nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dfki-nlp/","section":"authors","summary":"DFKI-NLP is a Natural Language Processing group of researchers, software engineers and students at the Berlin office of the German Research Center for Artificial Intelligence (DFKI) working on basic and applied research in areas covering, among others, information extraction, knowledge base population, dialogue, sentiment analysis, and summarization.","tags":null,"title":"DFKI-NLP","type":"authors"},{"authors":["marc-huebner"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb1a2f736e1bfeb288ac79f61fa27578","permalink":"https://dfki-nlp.github.io/authors/marc-huebner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marc-huebner/","section":"authors","summary":"","tags":["Alumni"],"title":"Marc HÃ¼bner","type":"authors"},{"authors":["nils-rethmeier"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5fa9f4aadfb6bfa3e5e6ad75c3611835","permalink":"https://dfki-nlp.github.io/authors/nils-rethmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nils-rethmeier/","section":"authors","summary":"","tags":["PhD Candidates"],"title":"Nils Rethmeier","type":"authors"},{"authors":["Yuxuan Chen","Jonas Mikkelsen","Arne Binder","Christoph Alt","Leonhard Hennig"],"categories":[],"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"71450b988e01035d8dda07fd4d7aebf8","permalink":"https://dfki-nlp.github.io/publication/acl2022-repl4nlp-chen-fewie/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/acl2022-repl4nlp-chen-fewie/","section":"publication","summary":"Pre-trained language models (PLM) are effective components of few-shot named entity recognition (NER) approaches when augmented with continued pre-training on task-specific out-of-domain data or fine-tuning on in-domain data. However, their performance in low-resource scenarios, where such data is not available, remains an open question. We introduce an encoder evaluation framework, and use it to systematically compare the performance of state-of-the-art pre-trained representations on the task of low-resource NER. We analyze a wide range of encoders pre-trained with different strategies, model architectures, intermediate-task fine-tuning, and contrastive learning. Our experimental results across ten benchmark NER datasets in English and German show that encoder performance varies significantly, suggesting that the choice of encoder for a specific low-resource scenario needs to be carefully evaluated.","tags":[],"title":"A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition","type":"publication"},{"authors":["David Harbecke","Yuxuan Chen","Leonhard Hennig","Christoph Alt"],"categories":[],"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"9cd7d48423cca6a139bf79bcc5d90221","permalink":"https://dfki-nlp.github.io/publication/acl2022-nlppower-harbecke-f1/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/acl2022-nlppower-harbecke-f1/","section":"publication","summary":"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F1, macro-F1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.","tags":[],"title":"Why only Micro-$F_1$? Class Weighting of Measures for Relation Classification","type":"publication"},{"authors":[],"categories":[],"content":"Two papers from DFKI-NLP authors have been accepted for publication at ACL 2022, the 60th Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Dublin, Ireland, from May 22nd through May 27th, 2022. One paper is on evaluating pre-trained encoders on the task of low-resource NER across several English and German datasets, the other analyzes relation classification evaluation and suggests that using F1 weightings other than micro-F1 tells us much more about model performance, e.g. on imbalanced datasets.\n  Yuxuan Chen, Jonas Mikkelsen, Arne Binder, Christoph Alt, Leonhard Hennig  (2022). A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition. ACL-REPL4NLP 2022.  PDF  Cite  Code  Project  Project    David Harbecke, Yuxuan Chen, Leonhard Hennig, Christoph Alt  (2022). Why only Micro-$F_1$? Class Weighting of Measures for Relation Classification. ACL-NLPPower 2022.  Cite  Project  Project   ","date":1648711441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648711441,"objectID":"011bca29552463724be0a68ec689af6a","permalink":"https://dfki-nlp.github.io/post/acl2022/","publishdate":"2022-03-31T09:24:01+02:00","relpermalink":"/post/acl2022/","section":"post","summary":"Two papers from DFKI-NLP authors have been accepted for publication at ACL 2022, the 60th Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Dublin, Ireland, from May 22nd through May 27th, 2022.","tags":[],"title":"2 papers to be presented at ACL 2022","type":"post"},{"authors":["Philippe Thomas","Leonhard Hennig"],"categories":[],"content":"","date":1642673791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642673791,"objectID":"427ce0221fcd0c6d0f6a0b3418cf8e0d","permalink":"https://dfki-nlp.github.io/project/bifold/","publishdate":"2022-01-20T11:16:31+01:00","relpermalink":"/project/bifold/","section":"project","summary":"BIFOLD conducts foundational research in big data management and machine learning, as well as its intersection, to educate future talent, and create high-impact knowledge exchange. The Berlin Institute for the Foundations of Learning and Data (BIFOLD), has evolved in 2019 from the merger of two national Artificial Intelligence Competence Centers: the Berlin Big Data Center (BBDC) and the Berlin Center for Machine Learning (BZML). Embedded in the vibrant Berlin metropolitan area, BIFOLD provides an outstanding scientific environment and numerous collaboration opportunities for national and international researchers. BIFOLD offers a broad range of research topics as well as a platform for interdisciplinary research and knowledge exchange with the sciences and humanities, industry, startups and society. Within BIFOLD, DFKI SLT conducts research in Clinical AI, specifically addressing the task of Pharmacovigilance. Pharmacovigilance is concerned with the assessment and prevention of adverse drug reactions (ADR) in pharmaceutical products. As the level of medication is generally raising all over the world, the potential risk of unwanted side effects, such as ADRs, is constantly increasing. Patients exchange views in their own language as 'experts in their own right,' in social media and disease-specific forums. Our project addresses the detection and extraction of ADR from medical forums and social media across different languages using cross-lingual transfer learning in combination with external knowledge sources.","tags":["Information Extraction"],"title":"BIFOLD","type":"project"},{"authors":["Steffen Castle","Robert Schwarzenberg","Mohsen Pourvali"],"categories":[],"content":"","date":1634256e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256e3,"objectID":"0909afe8749d563f9e6c0ad01633bcb5","permalink":"https://dfki-nlp.github.io/publication/nlpcc-castle/","publishdate":"2021-11-22T11:20:32+01:00","relpermalink":"/publication/nlpcc-castle/","section":"publication","summary":"Detecting when there is a domain drift between training and inference data is important for any model evaluated on data collected in real time. Many current data drift detection methods only utilize input features to detect domain drift. While effective, these methods disregard the modelâ€™s evaluation of the data, which may be a significant source of information about the data domain. We propose to use information from the model in the form of explanations, specifically gradient times input, in order to utilize this information. Following the framework of Rabanser et al. [11], we combine these explanations with two-sample tests in order to detect a shift in distribution between training and evaluation data. Promising initial experiments show that explanations provide useful information for detecting shift, which potentially improves upon the current state-of-the-art.","tags":[],"title":"Detecting Covariate Drift with Explanations","type":"publication"},{"authors":["Leonhard Hennig","Phuc Tran Truong","Aleksandra Gabryszak"],"categories":[],"content":"","date":1630972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630972800,"objectID":"b8425af8dc50ee6b2cd49c19ffc86d1e","permalink":"https://dfki-nlp.github.io/publication/konvens2021-hennig-mobie/","publishdate":"2021-08-11T00:00:00Z","relpermalink":"/publication/konvens2021-hennig-mobie/","section":"publication","summary":"We present MobIE, a German-language dataset, which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,232 social media texts and traffic reports with 91K tokens, and contains 20.5K annotated entities, 13.1K of which are linked to a knowledge base. A subset of the dataset is human-annotated with seven mobility-related, n-ary relation types, while the remaining documents are annotated using a weakly-supervised labeling approach implemented with the Snorkel framework. To the best of our knowledge, this is the first German-language dataset that combines annotations for NER, EL and RE, and thus can be used for joint and multi-task learning of these fundamental information extraction tasks. We make MobIE public at https://github.com/dfki-nlp/mobie.","tags":[],"title":"MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain","type":"publication"},{"authors":["Malte Ostendorff","Elliott Ash","Terry Ruas","Bela Gipp","Julian Moreno-Schneider","Georg Rehm"],"categories":[],"content":"","date":1622110683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622110683,"objectID":"ab6a46956823e641886d1cbdd6c30ba3","permalink":"https://dfki-nlp.github.io/publication/ostendorff2021/","publishdate":"2021-05-27T12:18:03+02:00","relpermalink":"/publication/ostendorff2021/","section":"publication","summary":"Recommender systems assist legal professionals in finding relevant literature for supporting their case. Despite its importance for the profession, legal applications do not reflect the latest advances in recommender systems and representation learning research. Simultaneously, legal recommender systems are typically evaluated in small-scale user study without any public available benchmark datasets. Thus, these studies have limited reproducibility. To address the gap between research and practice, we explore a set of state-of-the-art document representation methods for the task of retrieving semantically related US case law. We evaluate text-based (e.g., fastText, Transformers), citation-based (e.g., DeepWalk, PoincarÃ©), and hybrid methods. We compare in total 27 methods using two silver standards with annotations for 2,964 documents. The silver standards are newly created from Open Case Book and Wikisource and can be reused under an open license facilitating reproducibility. Our experiments show that document representations from averaged fastText word vectors (trained on legal corpora) yield the best results, closely followed by PoincarÃ© citation embeddings. Combining fastText and PoincarÃ© in a hybrid manner further improves the overall result. Besides the overall performance, we analyze the methods depending on document length, citation count, and the coverage of their recommendations. We make our source code, models, and datasets publicly available at this https URL. ","tags":[],"title":"Evaluating Document Representations for Content-based Legal Literature Recommendations","type":"publication"},{"authors":["Leonhard Hennig","Christoph Alt"],"categories":[],"content":"","date":1614075782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614075782,"objectID":"a8921d49553043a8c438729c3f6507f8","permalink":"https://dfki-nlp.github.io/project/cora4nlp/","publishdate":"2021-02-23T11:23:02+01:00","relpermalink":"/project/cora4nlp/","section":"project","summary":"Language is implicit - it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains, and transfer to, and between, both. Current language understanding technology, however, focuses on high resource languages and domains, uses little to no context, and assumes static data, task, and target distributions. The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project DEEPLEE and is carried out jointly between the language technology research departments in Berlin and SaarbrÃ¼cken. ","tags":["Information Extraction","Language Understanding"],"title":"Cora4NLP","type":"project"},{"authors":["Leonhard Hennig"],"categories":[],"content":"","date":1614075391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614075391,"objectID":"cbc4ff3659230581c9808dc981535447","permalink":"https://dfki-nlp.github.io/project/bbdc2/","publishdate":"2021-02-23T11:16:31+01:00","relpermalink":"/project/bbdc2/","section":"project","summary":"In order to optimally prepare industry, science and the society in Germany and Europe for the global Big Data trend, highly coordinated activities in research, teaching, and technology transfer regarding the integration of data analysis methods and scalable data processing are required. To achieve this, the Berlin Big Data Center is pursuing the following seven objectives: 1) Pooling expertise in scalable data management, data analytics, and big data application 2) Conducting fundamental research to develop novel and automatically scalable technologies capable of performing 'Deep Analysis' of 'Big Data'. 3) Developing an integrated, declarative, highly scalable open-source system that enables the specification, automatic optimization, parallelization and hardware adaptation, and fault-tolerant, efficient execution of advanced data analysis problems, using varying methods (e.g., drawn from machine learning, linear algebra, statistics and probability theory, computational linguistics, or signal processing), leveraging our work on Apache Flink 4) Transfering technology and know-how to support innovation in companies and startups. 5) Educating data scientists with respect to the five big data dimensions (i.e., applications, economic, legal, social, and technological) via leading educational programs. 6) Empowering people to leverage 'Smart Data', i.e., to discover newfound information based on their massive data sets. 7)Enabling the general public to conduct sound data-driven decision-making.","tags":["Information Extraction"],"title":"BBDC2","type":"project"},{"authors":["sven-schmeier","Christoph Alt","Robert Schwarzenberg"],"categories":[],"content":"","date":1614075391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614075391,"objectID":"c389619c2f6d4f10d79bd3a7581e3344","permalink":"https://dfki-nlp.github.io/project/deeplee/","publishdate":"2021-02-23T11:16:31+01:00","relpermalink":"/project/deeplee/","section":"project","summary":"The research work in DEEPLEE, which is carried out in the Language Technology research departments in SaabrÃ¼cken and Berlin, builds on DFKI's expertise in the areas of deep learning (DL) and language technology (LT) and develops it further. They aim for profound improvements of DL approaches in LT by focusing on four central, open research topics: Modularity in DNN architectures, Use of external knowledge, DNNs with explanation functionality, Machine Teaching Strategies for DNNs","tags":["Information Extraction","Language Understanding"],"title":"DEEPLEE","type":"project"},{"authors":["Leonhard Hennig","Christoph Alt"],"categories":[],"content":"","date":1614075391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614075391,"objectID":"cfeff35232a191ef8a348b8c96979ab5","permalink":"https://dfki-nlp.github.io/project/plass/","publishdate":"2021-02-23T11:16:31+01:00","relpermalink":"/project/plass/","section":"project","summary":"The aim of the PLASS project is to develop a prototypical B2B platform for AI-based decision support for supply chain management. The focus is on the automatic recognition of decision-relevant information and the acquisition of structured knowledge from global and multilingual text sources. These sources provide a large database for SCM information, especially for the early detection of critical events and risks, but also of opportunities, e.g. through new technologies, at suppliers and supply chains. PLASS enables SMEs and large companies to continuously monitor their suppliers and supply chains, and supports supply chain managers in risk assessment and decision-making.","tags":["Information Extraction","Low-Resource Learning"],"title":"PLASS","type":"project"},{"authors":["Philippe Thomas"],"categories":[],"content":"","date":1614075391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614075391,"objectID":"c25cf95b8ddec88b64c3b4a1a19063f6","permalink":"https://dfki-nlp.github.io/project/sim3s/","publishdate":"2021-02-23T11:16:31+01:00","relpermalink":"/project/sim3s/","section":"project","summary":"In the SIM3S project, data from the BMVI data offerings mCloud and MDM will be linked, refined and jointly analysed with other open data, user-generated content and data from individual modes of transport and other mobility-relevant companies in order to remove barriers and barriers to discrimination in everyday mobility. For the implementation of the project, state-of-the-art technologies and methods from the areas of Big Data Intelligent Analysis of mass data and artificial intelligence, in particular Natural Language Processing (NLP), are used.","tags":["Information Extraction"],"title":"SIM3S","type":"project"},{"authors":["Malte Ostendorff","Terry Ruas","Till Blume","Bela Gipp","Georg Rehm"],"categories":[],"content":"","date":1609064252,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609064252,"objectID":"9e79e1aa43e10a8c99ed53fd1f129c58","permalink":"https://dfki-nlp.github.io/publication/ostendorff2020c/","publishdate":"2020-12-27T12:17:32+02:00","relpermalink":"/publication/ostendorff2020c/","section":"publication","summary":"Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity for research papers. Paper citations indicate the aspect-based similarity, i.e., the section title in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our results show SciBERT as the best performing system. A qualitative examination validates our quantitative results. Our findings motivate future research of aspect-based document similarity and the development of a recommender system based on the evaluated techniques. We make our datasets, code, and trained models publicly available. ","tags":[],"title":"Aspect-based Document Similarity for Research Papers","type":"publication"},{"authors":["Marc HÃ¼bner","Christoph Alt","Robert Schwarzenberg","Leonhard Hennig"],"categories":[],"content":"Definition Extraction systems are a valuable knowledge source for both humans and algorithms. In this paper we describe our submissions to the DeftEval shared task (SemEval-2020 Task 6), which is evaluated on an English textbook corpus. We provide a detailed explanation of our system for the joint extraction of definition concepts and the relations among them. Furthermore we provide an ablation study of our model variations and describe the results of an error analysis.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"325726696f4edad34e20c5582f171a49","permalink":"https://dfki-nlp.github.io/publication/semeval2020-huebner-defx/","publishdate":"2020-12-01T14:36:10+02:00","relpermalink":"/publication/semeval2020-huebner-defx/","section":"publication","summary":"We describe our submissions to the DeftEval shared task (SemEval-2020 Task 6)","tags":[],"title":"Defx at SemEval-2020 Task 6: Joint Extraction of Concepts and Relations for Definition Extraction","type":"publication"},{"authors":["Malte Ostendorff","Terry Ruas","Moritz Schubotz","Georg Rehm","Bela Gipp"],"categories":[],"content":"","date":1598523462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598523462,"objectID":"4397f1dd4f22b53f0586d6677062f408","permalink":"https://dfki-nlp.github.io/publication/ostendorff2020/","publishdate":"2020-08-22T12:17:42+02:00","relpermalink":"/publication/ostendorff2020/","section":"publication","summary":"Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph-Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of recommender systems based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another. ","tags":[],"title":"Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles","type":"publication"},{"authors":["Robert Schwarzenberg","Steffen Castle"],"categories":[],"content":"","date":1595980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595980800,"objectID":"ca2fe3ec7f7c5501639381082cb399a5","permalink":"https://dfki-nlp.github.io/publication/icml-schwarzenberg-castle/","publishdate":"2020-08-26T14:09:16+02:00","relpermalink":"/publication/icml-schwarzenberg-castle/","section":"publication","summary":"Integrated Gradients (IG) and PatternAttribution (PA) are two established explainability methods for neural networks. Both methods are theoretically well-founded. However, they were designed to overcome different challenges. In this work, we combine the two methods into a new method, Pattern-Guided Integrated Gradients (PGIG). PGIG inherits important properties from both parent methods and passes stress tests that the originals fail. In addition, we benchmark PGIG against nine alternative explainability approaches (including its parent methods) in a large-scale image degradation experiment and find that it outperforms all of them.","tags":[],"title":"Pattern-Guided Integrated Gradients","type":"publication"},{"authors":["Hanchu Zhang","Leonhard Hennig","Christoph Alt","Changjian Hu","Yao Meng","Chao Wang"],"categories":[],"content":"","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"d04a7113f73244eb9fc10ce17aa6eb39","permalink":"https://dfki-nlp.github.io/publication/acl-ecnlp2020-zhang-bootstrapping/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/publication/acl-ecnlp2020-zhang-bootstrapping/","section":"publication","summary":"In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.","tags":[],"title":"Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning","type":"publication"},{"authors":["David Harbecke","Christoph Alt"],"categories":[],"content":"","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"015c496995d3ee9b3de39d1f0d271c69","permalink":"https://dfki-nlp.github.io/publication/acl-srw2020-harbecke-considering/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/publication/acl-srw2020-harbecke-considering/","section":"publication","summary":"Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model's decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models. Furthermore, gradient-based explanation methods disregard the discrete distribution of data in NLP. Thus, we propose OLM: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood, given the context of the original input. We lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in NLP and provide results that underline the importance of considering data likelihood in occlusion-based explanation.","tags":["Explainability"],"title":"Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling","type":"publication"},{"authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"categories":[],"content":"","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"8c459d2cf0ab9b1905c893ec91b8871c","permalink":"https://dfki-nlp.github.io/publication/acl2020-alt-probing/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/publication/acl2020-alt-probing/","section":"publication","summary":"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.","tags":[],"title":"Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction","type":"publication"},{"authors":["Christoph Alt","Aleksandra Gabryszak","Leonhard Hennig"],"categories":[],"content":"","date":1594339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594339200,"objectID":"4b6a4a0f754ff65730e25ac13406e529","permalink":"https://dfki-nlp.github.io/publication/acl2020-alt-tacred-revisited/","publishdate":"2020-07-10T00:00:00Z","relpermalink":"/publication/acl2020-alt-tacred-revisited/","section":"publication","summary":"TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","tags":[],"title":"TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"55804a630a9c47f344ed7ef49cca65d6","permalink":"https://dfki-nlp.github.io/dataset/dfki-mobie-corpus/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/dataset/dfki-mobie-corpus/","section":"dataset","summary":"This repository contains the DFKI MobIE Corpus (formerly \"DAYSTREAM Corpus\"), a dataset of 3,232 German-language documents collected between May 2015 - Apr 2019 that have been annotated with fine-grained geo-entities, such as location-street, location-stop and location-route, as well as standard named entity types (organization, date, number, etc). All location-related entities have been linked to either Open Street Map identifiers or database ids of Deutsche Bahn / Rhein-Main-Verkehrsverbund. The corpus has also been annotated with a set of 7 traffic-related n-ary relations and events, such as Accidents, Traffic jams, and Canceled Routes. It consists of Twitter messages, and traffic reports from e.g. radio stations, police and public transport providers. It allows for training and evaluating both named entity recognition algorithms that aim for fine-grained typing of geo-entities, entity linking of these entities, as well as n-ary relation extraction systems. You can find the description of the corpus here: https://www.dfki.de/web/forschung/projekte-publikationen/publikationen-uebersicht/publikation/11741/","tags":["Language Understanding","Information Extraction","Mobility"],"title":"MobIE Corpus","type":"dataset"},{"authors":["Karolina Zaczynska","Nils Feldhus","Robert Schwarzenberg","Aleksandra Gabryszak","Sebastian MÃ¶ller"],"categories":[],"content":"","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"69575402fe2a9bf17cf5ea06cff1b67c","permalink":"https://dfki-nlp.github.io/publication/swisstext2020-zaczynska-evaluating/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/publication/swisstext2020-zaczynska-evaluating/","section":"publication","summary":"Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most stateof-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits.","tags":[],"title":"Evaluating German Transformer Language Models with Syntactic Agreement Tests","type":"publication"},{"authors":["Dmitrii Aksenov","Julian Moreno Schneider","Peter Bourgonje","Robert Schwarzenberg","Leonhard Hennig","Georg Rehm"],"categories":[],"content":"","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589587200,"objectID":"691b01834034e508889d3d916be8218c","permalink":"https://dfki-nlp.github.io/publication/lrec2020-aksenov-abstractive/","publishdate":"2020-05-16T00:00:00Z","relpermalink":"/publication/lrec2020-aksenov-abstractive/","section":"publication","summary":"We explore to what extent knowledge about the pre-trained language model that is used is beneficial for the task of abstractive summarization. To this end, we experiment with conditioning the encoder and decoder of a Transformer-based neural model on the BERT language model. In addition, we propose a new method of BERT-windowing, which allows chunk-wise processing of texts longer than the BERT window size. We also explore how locality modeling, i.e., the explicit restriction of calculations to the local context, can affect the summarization ability of the Transformer. This is done by introducing 2-dimensional convolutional self-attention into the first layers of the encoder. The results of our models are compared to a baseline and the state-of-the-art models on the CNN/Daily Mail dataset. We additionally train our model on the SwissText dataset to demonstrate usability on German. Both models outperform the baseline in ROUGE scores on two datasets and show its superiority in a manual qualitative analysis.","tags":[],"title":"Abstractive Text Summarization based on Language Model Conditioning and Locality Modeling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1576508108,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576508108,"objectID":"2fd1a9eee61a268e57a10ee4b5e09ea7","permalink":"https://dfki-nlp.github.io/dataset/dfki-product-corpus/","publishdate":"2019-12-16T15:55:08+01:00","relpermalink":"/dataset/dfki-product-corpus/","section":"dataset","summary":"The Product Corpus is a dataset of 174 English web pages and social media posts annotated for product and company named entities, and the relation CompanyProvidesProduct. The goal is to make extraction of non-standard, B2B products and relations from unstructured text easier and more reliable. The corpus is also annotated for coreference chains of companies and products.","tags":["Language Understanding","Information Extraction"],"title":"Product Corpus","type":"dataset"},{"authors":null,"categories":null,"content":"","date":1576508108,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576508108,"objectID":"88423b8faaa8c52b9ff3f68c29045e86","permalink":"https://dfki-nlp.github.io/dataset/dfki-smartdata-corpus/","publishdate":"2019-12-16T15:55:08+01:00","relpermalink":"/dataset/dfki-smartdata-corpus/","section":"dataset","summary":"The SmartData Corpus is a dataset of 2598 German-language documents which has been annotated with fine-grained geo-entities, such as streets, stops and routes, as well as standard named entity types. It has also been annotated with a set of 15 traffic- and industry-related n-ary relations and events, such as Accidents, Traffic jams, Acquisitions, and Strikes. The corpus consists of newswire texts, Twitter messages, and traffic reports from radio stations, police and railway companies. It allows for training and evaluating both named entity recognition algorithms that aim for fine-grained typing of geo-entities, as well as n-ary relation extraction systems.","tags":["Language Understanding","Information Extraction"],"title":"SmartData Corpus","type":"dataset"},{"authors":["Lisa Raithel","Robert Schwarzenberg"],"categories":[],"content":"","date":1569863398,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569863398,"objectID":"e8e32af7af385deca2d3740c85dea084","permalink":"https://dfki-nlp.github.io/publication/cl-nvc/","publishdate":"2019-09-30T18:09:58+01:00","relpermalink":"/publication/cl-nvc/","section":"publication","summary":"Recently, Neural Vector Conceptualization (NVC) was proposed as a means to interpret samples from a word vector space. For NVC, a neural model activates higher order concepts it recognizes in a word vector instance. To this end, the model first needs to be trained with a sufficiently large instance-to-concept ground truth, which only exists for a few languages. In this work, we tackle this lack of resources with word vector space alignment techniques: We train the NVC model on a high resource language and test it with vectors from an aligned word vector space of another language, without retraining or fine-tuning. A quantitative and qualitative analysis shows that the NVC model indeed activates meaningful concepts for unseen vectors from the aligned vector space. NVC thus becomes available for low resource languages for which no appropriate concept ground truth exists.","tags":[],"title":"Cross-lingual Neural Vector Conceptualization","type":"publication"},{"authors":["Robert Schwarzenberg","Marc HÃ¼bner","David Harbecke","Christoph Alt","Leonhard Hennig"],"categories":[],"content":"","date":1569476323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569476323,"objectID":"a473d0f5238b0e0755c80265a795c54a","permalink":"https://dfki-nlp.github.io/publication/layerwise-relevance-visualization-in-convolutional-text-graph-classifiers/","publishdate":"2019-09-26T07:38:43+02:00","relpermalink":"/publication/layerwise-relevance-visualization-in-convolutional-text-graph-classifiers/","section":"publication","summary":"Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.","tags":[],"title":"Layerwise Relevance Visualization in Convolutional Text Graph Classifiers","type":"publication"},{"authors":["Christoph Alt","Marc HÃ¼bner","Leonhard Hennig"],"categories":[],"content":"","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"584cb9534c1efd44745dff10b6121dc1","permalink":"https://dfki-nlp.github.io/publication/acl2019-alt-finetuning/","publishdate":"2019-08-26T14:09:16+02:00","relpermalink":"/publication/acl2019-alt-finetuning/","section":"publication","summary":"We show that generative language model pre-training combined with selective attention improves recall for long-tail relations in distantly supervised neural relation extraction.","tags":[],"title":"Fine-Tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","type":"publication"},{"authors":["Neslihan Iskender","Aleksandra Gabryszak","Tim Polzehl","Leonhard Hennig","Sebastian MÃ¶ller"],"categories":[],"content":"","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"c481a30f36434bd26ba43c2e41347fd9","permalink":"https://dfki-nlp.github.io/publication/qomex2019-iskender-crowd/","publishdate":"2019-06-24T14:09:16+02:00","relpermalink":"/publication/qomex2019-iskender-crowd/","section":"publication","summary":"We analyze the feasibility and appropriateness of micro-task crowdsourcing for evaluation of different summary quality characteristics and report an ongoing work on the crowdsourced evaluation of query-based extractive text summaries","tags":[],"title":"A Crowdsourcing Approach to Evaluate the Quality of Query-based Extractive Text Summaries","type":"publication"},{"authors":["Malte Ostendorff","Peter Bourgonje","Maria Berger","Julian Moreno Schneider","Georg Rehm","Bela Gipp"],"categories":[],"content":"","date":1558953138,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558953138,"objectID":"a37577f877550a6974c6b3374f48e5ea","permalink":"https://dfki-nlp.github.io/publication/ostendorff2019/","publishdate":"2019-05-27T12:32:18+02:00","relpermalink":"/publication/ostendorff2019/","section":"publication","summary":" In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available ","tags":[],"title":"Enriching BERT with Knowledge Graph Embedding for Document Classification","type":"publication"},{"authors":["Christoph Alt","Marc HÃ¼bner","Leonhard Hennig"],"categories":[],"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"d6c8a5f82560901c80529e3d2c9e706a","permalink":"https://dfki-nlp.github.io/publication/akbc2019-alt-improving/","publishdate":"2019-08-26T14:36:10+02:00","relpermalink":"/publication/akbc2019-alt-improving/","section":"publication","summary":"We show that transfer learning through generative language model pre-training improves supervised neural relation extraction, achieving new state-of-the-art performance on TACRED and SemEval 2010 Task 8.","tags":[],"title":"Improving Relation Extraction by Pre-Trained Language Representations","type":"publication"},{"authors":["Robert Schwarzenberg","Lisa Raithel","David Harbecke"],"categories":[],"content":"","date":1554224121,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554224121,"objectID":"2e97d4e5206e642fb4fe1c042c0171cc","permalink":"https://dfki-nlp.github.io/publication/nvc/","publishdate":"2019-04-02T17:55:21+01:00","relpermalink":"/publication/nvc/","section":"publication","summary":"Distributed word vector spaces are considered hard to interpret which hinders the understanding of natural language processing (NLP) models. In this work, we introduce a new method to interpret arbitrary samples from a word vector space. To this end, we train a neural model to conceptualize word vectors, which means that it activates higher order concepts it recognizes in a given vector. Contrary to prior approaches, our model operates in the original vector space and is capable of learning non-linear relations between word vectors and concepts. Furthermore, we show that it produces considerably less entropic concept activation profiles than the popular cosine similarity.","tags":[],"title":"Neural Vector Conceptualization for Word Vector Space Interpretation","type":"publication"},{"authors":["Robert Schwarzenberg","David Harbecke","Vivien Macketanz","Eleftherios Avramidis","Sebastian MÃ¶ller"],"categories":[],"content":"","date":1553791245,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553791245,"objectID":"704fbe147dc60525fe2463f016efdd9d","permalink":"https://dfki-nlp.github.io/publication/diamat/","publishdate":"2019-03-28T17:40:45+01:00","relpermalink":"/publication/diamat/","section":"publication","summary":"Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.","tags":[],"title":"Train, Sort, Explain: Learning to Diagnose Translation Models","type":"publication"},{"authors":["Roland Roller","Christoph Alt","Laura Seiffe","He Wang"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"433a17498e14b803ea57862a8b6aadd1","permalink":"https://dfki-nlp.github.io/publication/mex-an-information-extraction-platform-for-german-medical-text/","publishdate":"2019-08-26T14:36:19+02:00","relpermalink":"/publication/mex-an-information-extraction-platform-for-german-medical-text/","section":"publication","summary":"","tags":[],"title":"mEx - an Information Extraction Platform for German Medical Text","type":"publication"},{"authors":["David Harbecke","Robert Schwarzenberg","Christoph Alt"],"categories":[],"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"68f68d704af9c42e6dc03edd211ae360","permalink":"https://dfki-nlp.github.io/publication/learning-explanations-from-language-data/","publishdate":"2019-08-26T14:36:16+02:00","relpermalink":"/publication/learning-explanations-from-language-data/","section":"publication","summary":"PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.","tags":[],"title":"Learning Explanations From Language Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://dfki-nlp.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://dfki-nlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://dfki-nlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]